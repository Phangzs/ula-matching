{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b9db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8b7a9",
   "metadata": {},
   "source": [
    "# GS Matching Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8716ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_by_summing(arr, groups, axis=0):\n",
    "    groups = np.array(groups, dtype=int)\n",
    "    \n",
    "    # Check that the groups sum to the size along the given axis.\n",
    "    if groups.sum() != arr.shape[axis]: exit(1)\n",
    "    \n",
    "    # Compute the starting indices of each group.\n",
    "    indices = np.concatenate(([0], np.cumsum(groups)[:-1]))\n",
    "    \n",
    "    # Use np.add.reduceat to sum over the specified groups.\n",
    "    return np.add.reduceat(arr, indices, axis=axis)\n",
    "\n",
    "# TODO: Check randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1301c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaleShapleyAlgorithmQuota(P1, P2, quota, info=None):\n",
    "    m, n = P1.shape  # m = size of Group 2, n = size of Group 1\n",
    "    orig_m, orig_n = m, n  \n",
    "\n",
    "    # Convert from 1-indexed to 0-indexed\n",
    "    P1 = P1 - np.ones_like(P1)\n",
    "    P2 = P2 - np.ones_like(P2)\n",
    "    \n",
    "    # standard == True means quotas on rows; otherwise, quotas on columns.\n",
    "    standard = (quota.shape[0] == m)\n",
    "    quota = quota.flatten()  # keep as vector of replication counts\n",
    "    \n",
    "    # We will fill `info_rep` after replication; `col2orig` after we know n.\n",
    "    info_rep = None  # ADDED\n",
    "    # ---- replicate the side with quotas (unchanged logic) ----\n",
    "    if standard:    \n",
    "        P1 = np.repeat(P1, quota, axis=0)\n",
    "        P2 = np.repeat(P2, quota, axis=0)\n",
    "    else:\n",
    "        P1 = np.repeat(P1, quota, axis=1)\n",
    "        P2 = np.repeat(P2, quota, axis=1)\n",
    "\n",
    "    m, n = P1.shape  # New dimensions after replication\n",
    "\n",
    "    # replicate info if provided\n",
    "    if info is not None:\n",
    "        info = np.asarray(info, dtype=int).flatten()\n",
    "        if info.shape[0] != orig_m:\n",
    "            raise ValueError(\"`info` must have length equal to the number of rows before replication.\")\n",
    "        info_rep = np.repeat(info, quota, axis=0) if standard else info.copy()\n",
    "\n",
    "    # map each (possibly replicated) column index -> original column index\n",
    "    if standard:\n",
    "        # columns were not replicated\n",
    "        col2orig = np.arange(n, dtype=int)\n",
    "    else:\n",
    "        # columns were replicated; expand mapping using `quota` over original columns\n",
    "        col2orig = np.concatenate([np.full(q, j, dtype=int) for j, q in enumerate(quota)])\n",
    "\n",
    "    # Always let columns propose. In both cases, we build P1_T from P1.\n",
    "    P1_T = np.empty((m, n), dtype=int)\n",
    "    for col in range(n):\n",
    "        P1_T[:, col] = np.argsort(P1[:, col])\n",
    "\n",
    "    NumStages = 0\n",
    "    # For each (replicated) column, we store a pointer and a flag indicating if it should propose.\n",
    "    cols_assigned = [[-1, True] for _ in range(n)]\n",
    "    # For each row, we keep a list of columns that have proposed to it.\n",
    "    rows_assigned = [[] for _ in range(m)]\n",
    "    \n",
    "    collisions = True\n",
    "    while collisions:\n",
    "        collisions = False\n",
    "        NumStages += 1\n",
    "\n",
    "        # has_match[c]  : whether this original column currently holds >=1 match\n",
    "        # all_pos[c]    : all its matched rows have info>0\n",
    "        # all_zero[c]   : all its matched rows have info==0\n",
    "        if info_rep is not None:\n",
    "            num_orig_cols = col2orig.max() + 1\n",
    "            has_match = np.zeros(num_orig_cols, dtype=bool)\n",
    "            all_pos   = np.ones(num_orig_cols,  dtype=bool)\n",
    "            all_zero  = np.ones(num_orig_cols,  dtype=bool)\n",
    "            for r in range(m):\n",
    "                if rows_assigned[r]:\n",
    "                    j = rows_assigned[r][0]            # the (replicated) column currently holding row r\n",
    "                    oc = col2orig[j]                   # original column index\n",
    "                    has_match[oc] = True\n",
    "                    v = int(info_rep[r])\n",
    "                    if v > 0:\n",
    "                        all_zero[oc] = False\n",
    "                    elif v == 0:\n",
    "                        all_pos[oc] = False\n",
    "                    else:  # negative -> treat as \">0\"\n",
    "                        all_zero[oc] = False\n",
    "        else:\n",
    "            has_match = all_pos = all_zero = None  # no info => no special tie-breaking\n",
    "\n",
    "        # Each column that needs to propose does so by using its pointer into its ordering in P1_T.\n",
    "        for i in range(n):\n",
    "            if cols_assigned[i][1]:\n",
    "                cols_assigned[i][1] = False\n",
    "                cols_assigned[i][0] += 1\n",
    "                p = cols_assigned[i][0]\n",
    "                if p >= m:\n",
    "                    continue  # This column has exhausted its list.\n",
    "\n",
    "                # tie-break among equal-best rows at this preference level using `info`\n",
    "                # Only if the column already has >=1 match and we have `info`.\n",
    "                if info_rep is not None:\n",
    "                    oc = col2orig[i]\n",
    "                    if has_match is not None and has_match[oc]:\n",
    "                        # Current best preference value for this column at pointer p\n",
    "                        cand_row = P1_T[p, i]\n",
    "                        best_val = P1[cand_row, i]\n",
    "\n",
    "                        # Scan the tied block [p .. end of that value]\n",
    "                        best_q = p\n",
    "                        if all_pos[oc]:\n",
    "                            # All current matches have info>0 → prefer a row with info==0 if available\n",
    "                            q = p\n",
    "                            while q < m and P1[P1_T[q, i], i] == best_val:\n",
    "                                r = P1_T[q, i]\n",
    "                                if info_rep[r] == 0:\n",
    "                                    best_q = q\n",
    "                                    break\n",
    "                                q += 1\n",
    "                        elif all_zero[oc]:\n",
    "                            # All current matches have info==0 → prefer a row with info>0 if available\n",
    "                            q = p\n",
    "                            while q < m and P1[P1_T[q, i], i] == best_val:\n",
    "                                r = P1_T[q, i]\n",
    "                                if info_rep[r] > 0:\n",
    "                                    best_q = q\n",
    "                                    break\n",
    "                                q += 1\n",
    "                        # Mixed (both 0 and >0 present) → keep original order\n",
    "\n",
    "                        # If we found a better q inside the tied block, swap so pointer p picks it now\n",
    "                        if best_q != p:\n",
    "                            P1_T[p, i], P1_T[best_q, i] = P1_T[best_q, i], P1_T[p, i]\n",
    "\n",
    "                row = P1_T[p, i]\n",
    "                rows_assigned[row].append(i)\n",
    "\n",
    "        # Each row resolves collisions by keeping the best column (according to P2).\n",
    "        for i in range(m):\n",
    "            if len(rows_assigned[i]) <= 1:\n",
    "                continue\n",
    "\n",
    "            collisions = True\n",
    "            # Choose the best proposal (the column with the lowest P2[i][col]), and record ties\n",
    "            minRank, minCol, ties = P2[i][rows_assigned[i][0]], rows_assigned[i][0], 0\n",
    "        \n",
    "            for col in rows_assigned[i]:\n",
    "                if P2[i][col] < minRank:\n",
    "                    minRank, minCol, ties = P2[i][col], col, 0\n",
    "                elif P2[i][col] == minRank:\n",
    "                    ties += 1\n",
    "            \n",
    "            # For every other column that proposed, mark it to propose again.\n",
    "            for col in rows_assigned[i]:\n",
    "                if col != minCol:\n",
    "                    cols_assigned[col][1] = True\n",
    "                    \n",
    "            rows_assigned[i] = [minCol]\n",
    "    \n",
    "    # Build the match matrix from the final assignments.\n",
    "    Match = np.zeros_like(P1_T)\n",
    "    for row in range(m):\n",
    "        for assigned in rows_assigned[row]:\n",
    "            Match[row][assigned] = 1\n",
    "\n",
    "    Match = combine_by_summing(Match, quota, axis=0) if standard else combine_by_summing(Match, quota, axis=1)\n",
    "    return Match, NumStages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eb4da",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60fb1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename):\n",
    "    # Load the CSV file (first row is header)\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Get column names\n",
    "    all_columns = df.columns.tolist()\n",
    "\n",
    "    # column G (7th column) = index 6 (0-indexed),\n",
    "    # column T (20th column) = index 19.\n",
    "\n",
    "\n",
    "    # col[len(\"Course Preferences\")+2:-1]\n",
    "    matching_columns = [col for col in df.columns if re.search(\"Course Preferences\", col)] # This assumes that there is a closing bracket after the course\n",
    "    if matching_columns == None: exit(1)\n",
    "\n",
    "    choice_columns = matching_columns\n",
    "\n",
    "\n",
    "\n",
    "    # Now filter the DataFrame to only include the desired columns:\n",
    "    # \"ID\", the choice columns, \"First Choice\", and \"Second Choice\"\n",
    "    cols_to_keep = [\"ID\"] + choice_columns + [\"First Choice\", \"Second Choice\"]\n",
    "    df_filtered = df[cols_to_keep]\n",
    "\n",
    "    # Prepare a new matrix: one row per ID and one column per choice column (G–T)\n",
    "    # Initialize with zeros (assuming that if neither choice matches, the value stays 0)\n",
    "    matrix = np.zeros((df_filtered.shape[0], len(choice_columns)), dtype=int)\n",
    "\n",
    "    # Pre-populate the matrix with (first character as integer + 2) for each cell in columns G-T\n",
    "    for i, row in df_filtered.iterrows():\n",
    "        for j, col in enumerate(choice_columns):\n",
    "            try:\n",
    "                matrix[i, j] = int(str(row[col]).strip()[0]) + 2\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "    \n",
    "    # Make sure normalized columns contain only the normalized text for matching\n",
    "    normalized_cols = [c[20:-1].strip().lower() for c in choice_columns]\n",
    "    normalized_cols = [c for c in normalized_cols if c]\n",
    "    # print(normalized_cols)\n",
    "\n",
    "    # Process each row of the filtered DataFrame.\n",
    "    # For each row, find the index (in choice_columns) for the first and second choices\n",
    "    # and assign 1 or 2 accordingly.\n",
    "    for idx, row in df_filtered.iterrows():\n",
    "        if not pd.isnull(row[\"First Choice\"]): first_choice = row[\"First Choice\"].strip().lower()\n",
    "        if not pd.isnull(row[\"Second Choice\"]): second_choice = row[\"Second Choice\"].strip().lower()\n",
    "        \n",
    "        # Set value 1 for first choice if the header exists in choice_columns\n",
    "        if first_choice in normalized_cols:\n",
    "            col_index = normalized_cols.index(first_choice)\n",
    "            matrix[idx, col_index] = 1\n",
    "        \n",
    "        # Set value 2 for second choice if the header exists in choice_columns\n",
    "        if second_choice in normalized_cols:\n",
    "            col_index = normalized_cols.index(second_choice)\n",
    "            matrix[idx, col_index] = 2\n",
    "\n",
    "    # print(df_filtered[df_filtered.columns[0]])\n",
    "    ids = np.array(df_filtered[df_filtered.columns[0]])\n",
    "    # print(ids, ids.shape)\n",
    "    # print(\"Matrix before:\", matrix)\n",
    "    # print(matrix.shape)\n",
    "    # np.column_stack((ids, matrix))\n",
    "    matrix = np.insert(matrix, 0, ids, axis=1)\n",
    "    matrix = np.insert(matrix, matrix.shape[1], df[\"Previous LA Service for CS\"].to_numpy(), axis=1)\n",
    "    # matrix = np.append(ids, matrix).reshape((matrix.shape[0], matrix.shape[1]+1))\n",
    "    # np.concatenate(ids, matrix)\n",
    "    # np.insert(matrix, range(matrix.shape[0]), ids)\n",
    "    # print(\"matrix: \", matrix, matrix.shape)\n",
    "    # for row in range(len(matrix)):\n",
    "\n",
    "\n",
    "    # # dataframe if needed\n",
    "    # result_df = pd.DataFrame(matrix, columns=choice_columns)\n",
    "    return matrix, df_filtered, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23439ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prof(filename):\n",
    "    df = pd.read_csv(filename, index_col=False).fillna(99)\n",
    "\n",
    "\n",
    "    # Convert from floats to ints\n",
    "    float_cols = df.select_dtypes(include=['float']).columns\n",
    "    df[float_cols] = df[float_cols].astype(int)\n",
    "\n",
    "\n",
    "    # Get quotas from column\n",
    "    prof_quotas = df[df.columns[2]]\n",
    "    prof_quotas = prof_quotas.to_numpy()[1:]\n",
    "\n",
    "\n",
    "    # Sort list and eliminate non-courses\n",
    "    from natsort import natsorted\n",
    "    sorted_list = natsorted(prof_quotas.tolist())\n",
    "    prof_quotas = np.array(sorted_list, dtype=prof_quotas.dtype)\n",
    "    prof_quotas = [s for s in prof_quotas if s[0].isdigit()]\n",
    "\n",
    "\n",
    "    # Make unique array\n",
    "    quota_array = []\n",
    "    curr = ''\n",
    "    for code in prof_quotas:\n",
    "        if curr != code: \n",
    "            quota_array.append(0)\n",
    "            curr = code\n",
    "        quota_array[-1] += 1\n",
    "\n",
    "    \n",
    "\n",
    "    df_formatted = df\n",
    "    \n",
    "    # Get only desired columns (in-place)\n",
    "    df_formatted = df_formatted.iloc[:, np.r_[0, 3:df_formatted.shape[1]]] \n",
    "\n",
    "    df_formatted = df_formatted[1:]\n",
    "    df_formatted = np.array(df_formatted, dtype=int)\n",
    "\n",
    "    return df_formatted, np.array(quota_array).reshape((len(quota_array),1)), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cafd9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyEmail(prof_name, dictionary):\n",
    "    try:\n",
    "        email = dictionary[prof_name]\n",
    "    except:\n",
    "        print(f\"No email for {prof_name}!\")\n",
    "        return prof_name, None\n",
    "    return prof_name, email\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def courseTopChoices(df, export_path=\"\"):\n",
    "    # 1. Find First and Second Choice columns (should be consecutive)\n",
    "    # 2. For each possible course (find start and end of course name columns)\n",
    "    # 2.5. Create email dictionary\n",
    "    # 3. Filter by those values per course, create a spreadsheet\n",
    "    # 4. Return spreadsheets (OR PRODUCE CSV FILES?)\n",
    "\n",
    "    # First Choice name\n",
    "    first_choice_column_name = \"First Choice\"\n",
    "    course_string = \"Course Preferences\"\n",
    "\n",
    "    # Part 1\n",
    "\n",
    "    first_choice_index = df.columns.get_loc(first_choice_column_name)\n",
    "\n",
    "    # Part 2\n",
    "\n",
    "    # Find column of all occurance of courses\n",
    "    matching_columns = [col[len(course_string)+2:-1] for col in df.columns if re.search(course_string, col)] # This assumes that there is a closing bracket after the course\n",
    "    if matching_columns == None: exit(1)\n",
    "\n",
    "\n",
    "    # Part 2.5 \n",
    "    data = pd.read_csv(\"email_directory.csv\")\n",
    "    data = dict(zip(data['Professor'], data['email']))\n",
    "\n",
    "\n",
    "    # Part 3 & 4\n",
    "\n",
    "    # First make sure professor emails exist in the directory\n",
    "    matched_emails = []\n",
    "    unmatched_names = []\n",
    "    for course in matching_columns:\n",
    "        prof = course[course.find(\"(\")+1:course.find(\")\")]\n",
    "        \n",
    "        name, email = verifyEmail(prof, data)\n",
    "\n",
    "        if email == None: unmatched_names.append(name)\n",
    "        else: matched_emails.append((name, email))\n",
    "\n",
    "    # if len(unmatched_names) != 0:\n",
    "    #     exit(1)\n",
    "\n",
    "    # Then generate the spreadsheets\n",
    "    for course in matching_columns:\n",
    "\n",
    "        course_df = df[(df['First Choice'] == course) | (df['Second Choice'] == course)]\n",
    "\n",
    "        course_df.to_csv(export_path+course+'.csv', index=False)\n",
    "\n",
    "        name, email = verifyEmail(prof, data)\n",
    "\n",
    "        # TODO: Email CSV File\n",
    "\n",
    "        # # Import smtplib for the actual sending function\n",
    "        # import smtplib\n",
    "\n",
    "        # # Import the email modules we'll need\n",
    "        # from email.message import EmailMessage\n",
    "\n",
    "        # # Open the plain text file whose name is in textfile for reading.\n",
    "        # textfile = \"test.txt\"\n",
    "        # with open(textfile) as fp:\n",
    "        #     # Create a text/plain message\n",
    "        #     msg = EmailMessage()\n",
    "        #     msg.set_content(fp.read())\n",
    "\n",
    "        # # me == the sender's email address\n",
    "        # # you == the recipient's email address\n",
    "        # msg['Subject'] = f'The contents of {textfile}'\n",
    "        # msg['From'] = \"erikfeng@ucsb.edu\"\n",
    "        # msg['To'] = \"erikfeng@ucsb.edu\"\n",
    "\n",
    "        # # Send the message via our own SMTP server.\n",
    "        # s = smtplib.SMTP('localhost')\n",
    "        # s.send_message(msg)\n",
    "        # s.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3413f5",
   "metadata": {},
   "source": [
    "# Iterative Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91434b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchMatrices(P1, P2):\n",
    "\n",
    "    ### FILTER MATRICES TO COMMON IDS\n",
    "    # 1) find the common IDs\n",
    "    common_ids = np.intersect1d(P1[:,0], P2[:,0])\n",
    "    # common_ids == array([10, 20])\n",
    "\n",
    "    # 2) build boolean masks for each matrix\n",
    "    maskA = np.isin(P1[:,0], common_ids)\n",
    "    maskB = np.isin(P2[:,0], common_ids)\n",
    "\n",
    "    # 3) filter\n",
    "    P1 = P1[maskA]\n",
    "    P2 = P2[maskB]\n",
    "\n",
    "    ### SORT MATRICES\n",
    "    # 1) pull out the ID columns\n",
    "    ids1 = P1[:, 0]\n",
    "    ids2 = P2[:, 0]\n",
    "\n",
    "    # print(ids1, ids2)\n",
    "\n",
    "    # 2) build a lookup from ID → row‐index in mat2\n",
    "    pos_in_P2 = { id_: i for i, id_ in enumerate(ids2) }\n",
    "\n",
    "    # 3) for each id in mat1, find its row in mat2\n",
    "    order = [pos_in_P2[id_] for id_ in ids1]\n",
    "\n",
    "    # 4) fancy‐index to reorder mat2\n",
    "    P2_matched = P2[order]\n",
    "\n",
    "    # print(P2_matched)\n",
    "    return P1, P2_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efba49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyMaxProbMatch(dmatch, quotas):\n",
    "    final_match = np.zeros_like(dmatch[:, 1:])\n",
    "\n",
    "    dmatch_temp = dmatch.copy()[:, 1:]\n",
    "\n",
    "    temp_quotas = quotas.copy()\n",
    "\n",
    "    while sum(temp_quotas) > 0:\n",
    "        \n",
    "        max_index = np.argmax(dmatch_temp)\n",
    "\n",
    "        row_index, col_index = np.unravel_index(max_index, final_match.shape)\n",
    "\n",
    "        # print(max_index, dmatch_temp)\n",
    "\n",
    "        dmatch_temp[row_index, col_index] = 0\n",
    "\n",
    "        # print(np.sum(final_match[row_index]), temp_quotas[col_index][0])\n",
    "        if np.sum(final_match[row_index]) > 0 or temp_quotas[col_index][0] <= 0: continue\n",
    "\n",
    "        final_match[row_index, col_index] = 1\n",
    "\n",
    "        \n",
    "        temp_quotas[col_index][0] -= 1\n",
    "        # print(temp_quotas, temp_quotas[col_index])\n",
    "    \n",
    "    return np.insert(final_match, 0, dmatch[:, 0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11980d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0db94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erik/.local/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:5596: RuntimeWarning: invalid value encountered in cast\n",
      "  values = array(values, copy=None, ndmin=arr.ndim, dtype=arr.dtype)\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:04<00:00, 2339.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg incorrect: 165126\n",
      "avg avg total diff: 16.5126\n",
      "avg avg diff: 0.1260503816793893\n",
      "\n",
      "\n",
      "ALL PAIRED MATCHES:\n",
      "56764 5A\n",
      "201482 8W\n",
      "246205 111\n",
      "392006 130A\n",
      "282467 130B\n",
      "286479 5A\n",
      "450682 5B\n",
      "12715 5A\n",
      "358604 64\n",
      "437393 16\n",
      "353696 154\n",
      "532540 130B\n",
      "226728 24\n",
      "326744 154\n",
      "124908 130B\n",
      "121163 5A\n",
      "553587 5A\n",
      "452520 130A\n",
      "586513 8W\n",
      "268654 8W\n",
      "504594 5A\n",
      "533793 16\n",
      "392802 40\n",
      "155573 156\n",
      "68873 9\n",
      "80296 16\n",
      "236703 154\n",
      "242174 24\n",
      "238731 16\n",
      "345323 130B\n",
      "611178 16\n",
      "635894 9\n",
      "218710 16\n",
      "534294 16\n",
      "339740 9\n",
      "244938 40\n",
      "621851 40\n",
      "245104 154\n",
      "299050 8W\n",
      "99385 24\n",
      "576594 148\n",
      "522736 5B\n",
      "22665 16\n",
      "14993 130B\n",
      "561779 16\n",
      "171863 16\n",
      "447435 5B\n",
      "453077 154\n",
      "578302 111\n",
      "46173 64\n",
      "24997 148\n",
      "431873 5A\n",
      "492542 130B\n",
      "\n",
      "\n",
      "MATCHING PERFORMANCE RELATIVE TO GIVEN\n",
      "Identical Matches: 94, Total Matches: 131\n",
      "Relative Identical Matches: 71.76%\n",
      "\n",
      "Skipped matches: {65: 'W8'}\n",
      "Proportion of mixed exp classes: 58.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "P2, quotas, df_prof = load_prof(\"Example_ULA_Applications_W24_2.csv\")\n",
    "# print(P2)\n",
    "# print(\"QUOTAS\", quotas)\n",
    "# exit(0)\n",
    "\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(load_csv(\"Example_ULA_Applications.csv\")[0])\n",
    "\n",
    "\n",
    "# exit(0)\n",
    "\n",
    "P1, df_filtered, df_original = load_csv(\"Example_ULA_Applications_W24.csv\")\n",
    "\n",
    "\n",
    "# print(P1.shape[0])\n",
    "# test_quotas = np.ones((P1.shape[0],1)).astype(int) * 3\n",
    "# test_quotas = np.ones((1,P1.shape[1])).astype(int) * 3\n",
    "test_quotas = quotas\n",
    "# print(test_quotas)\n",
    "\n",
    "# courseTopChoices(df_original)\n",
    "\n",
    "# exit(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P1, P2 = matchMatrices(P1, P2)\n",
    "experience = P1[:, -1]\n",
    "\n",
    "# print(P1.shape, P2.shape, experience)\n",
    "# print(P1[0], P2[0])\n",
    "\n",
    "\n",
    "P1 = P1[:, :-1]\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Original Matrices: \\n\", P1.shape, P2.shape, test_quotas.shape)\n",
    "\n",
    "match1, _ = GaleShapleyAlgorithmQuota(P2[:, 1:], P1[:, 1:], quota=test_quotas, info=experience)\n",
    "\n",
    "# print(match1)\n",
    "\n",
    "# exit(0)\n",
    "\n",
    "\n",
    "p = np.random.permutation(P1.shape[0])\n",
    "\n",
    "\n",
    "incorrect_sum = 0\n",
    "incorrect = 0\n",
    "iterations = 10000\n",
    "total_match = np.zeros_like(P1)[:,1:]\n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "    p = np.random.permutation(P1.shape[0])\n",
    "    # print(p)\n",
    "    s = np.empty(p.size, dtype=np.int32)\n",
    "    for i in np.arange(p.size):\n",
    "        s[p[i]] = i\n",
    "\n",
    "    # print(\"s: \", s)\n",
    "\n",
    "    P1_rand, P2_rand = matchMatrices(P1[p], P2)\n",
    "\n",
    "    # print(np.all(P1_rand[s][:,0] == P1[:,0]))\n",
    "    # print(test_quotas,p)\n",
    "\n",
    "    \n",
    "    # print(P1, P2)\n",
    "    # print(P1[:, 1:].shape, P2[:, 1:].shape, test_quotas)\n",
    "\n",
    "    # match1, _ = GaleShapleyAlgorithmQuota(P2[:, 1:], P1[:, 1:], quota=test_quotas)\n",
    "    match2, _ = GaleShapleyAlgorithmQuota(P2_rand[:, 1:], P1_rand[:, 1:], quota=test_quotas, info=experience)\n",
    "\n",
    "    match2 = match2[s]\n",
    "\n",
    "\n",
    "    # with np.printoptions(threshold=np.inf):\n",
    "    #     # if i == 0:\n",
    "    #     #     print(P1-(P1[p])[s])\n",
    "    #     #     print(\"MATCH DIFF:\\n\\n\\n\\n\\n\\n:\", match1-match2)\n",
    "\n",
    "    #     if not np.all(np.abs(match1-match2) < 0.1): incorrect += 1\n",
    "    #     incorrect_sum += np.count_nonzero(match1-match2)\n",
    "\n",
    "    total_match = total_match + match2\n",
    "\n",
    "# TODO: Reliability in outcomes \"krpendorf's alpha(?)\"\n",
    "\n",
    "# print(\"Randomized Matrices: \\n\", P1_rand, \"\\n\", P2_rand)\n",
    "# print(\"Matchings:\")\n",
    "# print(np.insert(match1, 0, P1[:, 0], axis=1))\n",
    "# print(match2)\n",
    "\n",
    "diffused_matches = np.round(np.insert(total_match / iterations * 1000, 0, P1[:, 0], axis=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with np.printoptions(suppress=True,precision=1,threshold=np.inf):\n",
    "#     print(\"DIFFUSED MATCHES: \")\n",
    "#     print(diffused_matches)\n",
    "\n",
    "#     print(np.round(np.insert(total_match / iterations * 1000, 0, P1[:, 0]/100, axis=1)))\n",
    "#     # print(total_match)\n",
    "#     print(np.sum(np.round(total_match) == iterations))\n",
    "#     print(np.sum(np.round(total_match) >= iterations/2.0))\n",
    "# print(np.sum(total_match / iterations, axis=1))\n",
    "# print(np.sum(total_match / iterations, axis=0))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"MATCH DIFF:\\n\\n\\n\\n\")\n",
    "\n",
    "# print(P1_rand[:,1:].shape, P2_rand[:, 1:].shape, P1[:, 1:].shape, P2[:, 1:].shape)\n",
    "# print(test_quotas, P1_rand[:, 1:], P2_rand[:, 1:], P1[:, 1:], P2[:, 1:], s, sep=\"\\n\\n\")\n",
    "# print(\"incorrect:\", incorrect)\n",
    "# print(\"incorrect sum:\", incorrect_sum)\n",
    "# print(\"avg total diff:\", incorrect_sum *1.0 / iterations)\n",
    "# print(\"avg diff:\", incorrect_sum *1.0 / iterations / P1.shape[0])\n",
    "\n",
    "# print(test_quotas, P1_rand[:, 1:], P2_rand[:, 1:], P1[:, 1:], P2[:, 1:], s, sep=\"\\n\\n\")\n",
    "\n",
    "diff = match1 * iterations - total_match\n",
    "incorrect = np.sum(np.abs(diff))\n",
    "\n",
    "\n",
    "# print(match1 * iterations, \"\\n\", total_match)\n",
    "# print(\"diff\", diff)\n",
    "print(\"\\navg incorrect:\", incorrect)\n",
    "# print(\"avg incorrect sum:\", incorrect_sum)\n",
    "print(\"avg avg total diff:\", incorrect *1.0 / iterations)\n",
    "print(\"avg avg diff:\", incorrect *1.0 / iterations / P1.shape[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "with np.printoptions(suppress=True,precision=1):\n",
    "    maxMatch = greedyMaxProbMatch(diffused_matches, test_quotas)\n",
    "    # print(maxMatch)\n",
    "    # print(match1)\n",
    "\n",
    "\n",
    "# exit(0)\n",
    "\n",
    "# with np.printoptions(threshold=np.inf):\n",
    "#     print(match1, P1[:, 0])\n",
    "\n",
    "# print(np.count_nonzero(match1))\n",
    "\n",
    "# print(df_filtered)\n",
    "# print(np.nonzero(match[0])[0][0])\n",
    "\n",
    "# Don't want ID\n",
    "df_filtered_columns_list = df_filtered.columns.to_list()[1:]\n",
    "\n",
    "# print(\"ID:\", df_filtered.iloc[51][\"ID\"], \"Course:\", df_filtered_columns_list[np.nonzero(match[51])[0][0]][20:-1])\n",
    "# print(df_filtered_columns_list)\n",
    "counter = 0\n",
    "# print(\"MATCHES (ID, COURSE):\")\n",
    "\n",
    "match_dict = {}\n",
    "\n",
    "# print(df_filtered)\n",
    "for i in range(len(maxMatch)):\n",
    "    # Skip non-matched\n",
    "    # with np.printoptions(suppress=True,precision=1): print(maxMatch[i])\n",
    "    if np.all(maxMatch[i, 1:] == 0): continue\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    id = str(int(maxMatch[i,0]))\n",
    "    # print(maxMatch[i,0])\n",
    "    # print()\n",
    "    course_number = df_filtered_columns_list[np.nonzero(maxMatch[i, 1:])[0][0]][20:-1].split()[0]\n",
    "\n",
    "    # print(id, course_number)\n",
    "    match_dict[id] = course_number\n",
    "\n",
    "# print(df_prof.iloc[:, [0,2]]['ID'])\n",
    "# print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "experience_dict = dict(zip(df_original.iloc[:, 1], df_original.iloc[:, 2]))\n",
    "# print(\"EXP DICT:\", experience_dict)\n",
    "\n",
    "reference_dict = dict(zip(df_prof.iloc[1:, 0], df_prof.iloc[1:, 2]))\n",
    "print(\"ALL PAIRED MATCHES:\")\n",
    "for id in match_dict: print(id, match_dict[id])\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(\"REF DICT:\", reference_dict)\n",
    "\n",
    "# print('\\n\\n\\n')\n",
    "\n",
    "match_comparison = []\n",
    "correct = 0\n",
    "for reference in reference_dict:\n",
    "    # print(reference, type(reference))\n",
    "    aligned_match = reference, reference_dict.get(reference), match_dict.get(reference) if match_dict.get(reference) != None else \"-\"\n",
    "    match_comparison.append(aligned_match)\n",
    "    if aligned_match[1].lower().strip() == aligned_match[2].lower().strip(): \n",
    "        correct += 1\n",
    "    else:\n",
    "        continue\n",
    "        # print(aligned_match)\n",
    "    \n",
    "\n",
    "    # print(aligned_match)\n",
    "# print(df_prof.iloc[1:, [0,2]].to_dict())\n",
    "\n",
    "\n",
    "course_reference_inv = {}\n",
    "course_match_inv = {}\n",
    "course_exp_reference = {}\n",
    "course_exp_match = {}\n",
    "for course in set(reference_dict.values()): course_exp_reference[course] = [0, 1]\n",
    "for course in set(match_dict.values()): course_exp_match[course] = [0, 1]\n",
    "for key, value in reference_dict.items():\n",
    "    course_reference_inv.setdefault(value, []).append(key)\n",
    "for key, value in match_dict.items():\n",
    "    course_match_inv.setdefault(value, []).append(key)\n",
    "\n",
    "\n",
    "\n",
    "# for \n",
    "\n",
    "# for course in set(reference_dict.values()):\n",
    "#     if course == '-': continue\n",
    "#     for student in course_reference_inv.get(course):\n",
    "#         student = int(student)\n",
    "#         print(course, student, course_exp_reference.get(course), experience_dict.get(student), type(student))\n",
    "#         course_exp_reference.get(course)[0] += experience_dict.get(student)\n",
    "#         course_exp_reference.get(course)[1] *= experience_dict.get(student)\n",
    "    \n",
    "# for course in set(reference_dict.values()):\n",
    "#     print(course_exp_reference.get(course)[0] > 0, course_exp_reference.get(course)[0] == 0)\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"MATCHING PERFORMANCE RELATIVE TO GIVEN\")\n",
    "print(f\"Identical Matches: {correct}, Total Matches: {len(match_comparison)}\")\n",
    "print(f\"Relative Identical Matches: {round(correct/len(match_comparison)*100,2)}%\\n\")\n",
    "\n",
    "# print(maxMatch)\n",
    "# print(experience_dict)\n",
    "# print(reference_dict)\n",
    "\n",
    "# print(len(experience_dict), len(reference_dict))\n",
    "\n",
    "\n",
    "# Build dictionary of ULAs per class\n",
    "classes_dict = {}\n",
    "for id in reference_dict:\n",
    "    if reference_dict[id] not in classes_dict: classes_dict[reference_dict[id]] = [id]\n",
    "    else: classes_dict[reference_dict[id]].append(id)\n",
    "\n",
    "# Build dictionary of whether or not each class has a mixed-ULA experience (at least 1 new, 1 experienced)\n",
    "class_experience_dict = classes_dict.copy()\n",
    "avoided_ids = {}\n",
    "for class_id in class_experience_dict:\n",
    "    experienced, inexperienced = False, False\n",
    "\n",
    "    # print(class_experience_dict)\n",
    "\n",
    "    for student_id in class_experience_dict[class_id]:\n",
    "        student_id = int(student_id)\n",
    "        if student_id not in experience_dict:\n",
    "            try: avoided_ids[student_id] = reference_dict[student_id]\n",
    "            except: avoided_ids[student_id] = reference_dict[str(student_id)]\n",
    "            continue\n",
    "        experienced = experienced or experience_dict[student_id] > 0\n",
    "        inexperienced = inexperienced or experience_dict[student_id] <= 0\n",
    "\n",
    "    if experienced and inexperienced: class_experience_dict[class_id] = True\n",
    "    elif experienced: class_experience_dict[class_id] = \"No inexperienced\"\n",
    "    elif inexperienced: class_experience_dict[class_id] = \"No experienced\"\n",
    "    else: class_experience_dict[class_id] = False\n",
    "\n",
    "# print(class_experience_dict,\"\\n\")\n",
    "# print(reference_dict,\"\\n\")\n",
    "# print(experience_dict,\"\\n\")\n",
    "\n",
    "# print(list(class_experience_dict.values()).count(True))\n",
    "print(\"Skipped matches:\", avoided_ids)\n",
    "print(f\"Proportion of mixed exp classes: {round(list(class_experience_dict.values()).count(True) / len(class_experience_dict)*100, 2)}%\")\n",
    "    \n",
    "\n",
    "\n",
    "# print(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1d721",
   "metadata": {},
   "source": [
    "# Benchmarking time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3341d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_benchmark(num_emails: int, skip: int = 1, seeds: int = 1):\n",
    "    times = []\n",
    "    for counter in range(0, num_emails, skip):\n",
    "        email_tuples = []\n",
    "        for count in range(counter):\n",
    "            email_tuples.append((f\"erik({count})\", \"erikfeng16@gmail.com\"))\n",
    "\n",
    "        total_time = 0\n",
    "        for seed in range(seeds):\n",
    "            start_time = time.perf_counter()\n",
    "            send_emails(email_tuples, \"ula_rejection\", name_variable=\"applicant\")\n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            total_time += start_time-end_time\n",
    "\n",
    "        total_time /= seeds\n",
    "\n",
    "        times.append([counter, total_time])\n",
    "    return times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36c87bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "ula_rejection\n",
      "data: {'current_quarter': 'Winter 25', 'next_quarter': 'Spring 25'}\n",
      "LA positions for {{current_quarter}} have been filled\n",
      "\n",
      "[[0, -0.0007557708537206054], [1, -0.8356760622235015], [2, -2.115392249985598], [3, -2.9936077082995327], [4, -4.054755220771767], [5, -4.98212928341236], [6, -5.9715673831757154], [7, -6.958230858319439], [8, -8.149514154321514], [9, -9.079267374915071]]\n"
     ]
    }
   ],
   "source": [
    "# print(email_benchmark(10, seeds=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
